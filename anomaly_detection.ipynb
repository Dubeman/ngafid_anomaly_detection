{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from statsmodels.tsa.api import VAR\n",
    "import matplotlib.pyplot as plt\n",
    "from fastdtw import fastdtw\n",
    "from scipy.spatial.distance import euclidean\n",
    "from tqdm import tqdm\n",
    "\n",
    "FEATURES = ['volt1', 'volt2', 'amp1', 'amp2', 'FQtyL', 'FQtyR', 'E1 FFlow',\n",
    "            'E1 OilT', 'E1 OilP', 'E1 RPM', 'E1 CHT1', 'E1 CHT2', 'E1 CHT3',\n",
    "            'E1 CHT4', 'E1 EGT1', 'E1 EGT2', 'E1 EGT3', 'E1 EGT4', 'OAT', 'IAS',\n",
    "            'VSpd', 'NormAc', 'AltMSL']\n",
    "\n",
    "\n",
    "\n",
    "SHAPE = (4096, 23)  # Example shape, adjust as needed\n",
    "\n",
    "\n",
    "def get_dataset(df):\n",
    "    \"\"\"\n",
    "    Processes a DataFrame to create a PyTorch dataset.\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The input DataFrame containing sensor data. \n",
    "                               It must have columns 'id' and 'before_after'.\n",
    "    Returns:\n",
    "        torch.utils.data.TensorDataset: A PyTorch dataset containing the processed sensor data and labels.\n",
    "    The function performs the following steps:\n",
    "    1. Extracts unique IDs from the DataFrame.\n",
    "    2. For each unique ID, extracts the last SHAPE[0] rows of sensor data (first 23 columns).\n",
    "    3. Pads the sensor data if it has fewer than SHAPE[0] rows.\n",
    "    4. Converts the sensor data to a PyTorch tensor.\n",
    "    5. Extracts the 'before_after' label for each ID.\n",
    "    6. Stacks all sensor data tensors and labels into a single PyTorch dataset.\n",
    "    \"\"\"\n",
    "    ids = df.id.unique()\n",
    "\n",
    "    sensor_datas = []\n",
    "    afters = []\n",
    "\n",
    "    for id in tqdm(ids):\n",
    "        sensor_data = df[df.id == id].iloc[-SHAPE[0]:, :23].values\n",
    "        \n",
    "        sensor_data = np.pad(sensor_data, ((0, SHAPE[0] - len(sensor_data)), (0, 0)))\n",
    "\n",
    "        # Convert to PyTorch tensor\n",
    "        sensor_data = torch.tensor(sensor_data, dtype=torch.float32)\n",
    "\n",
    "        after = df[df.id == id]['before_after'].iloc[0]\n",
    "\n",
    "        sensor_datas.append(sensor_data)\n",
    "        afters.append(after)\n",
    "\n",
    "    # Stack the tensors\n",
    "\n",
    "\n",
    "    sensor_datas = torch.stack(sensor_datas)\n",
    "\n",
    "\n",
    "    afters = torch.tensor(afters, dtype=torch.float32)\n",
    "\n",
    "    # Create a PyTorch dataset\n",
    "    dataset = torch.utils.data.TensorDataset(sensor_datas, afters)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def fix_type(x, y):\n",
    "    return x.float(), y.float()\n",
    "\n",
    "def prepare_for_training(ds, shuffle=False, repeat=False, predict=True, aug=False, batch_size=32, model_type='short_lstm', model_loss_type='bce'):\n",
    "    \"\"\"\n",
    "    Prepares the dataset for training by applying various transformations such as shuffling, repeating, and batching.\n",
    "\n",
    "    Args:\n",
    "        ds (Dataset): The dataset to be prepared.\n",
    "        shuffle (bool, optional): If True, shuffles the dataset. Defaults to False.\n",
    "        repeat (bool, optional): If True, repeats the dataset. Defaults to False.\n",
    "        predict (bool, optional): If True, prepares the dataset for prediction. Defaults to True.\n",
    "        aug (bool, optional): If True, applies data augmentation. Defaults to False.\n",
    "        batch_size (int, optional): The size of the batches. Defaults to 32.\n",
    "        model_type (str, optional): The type of model being used. Defaults to 'short_lstm'.\n",
    "        model_loss_type (str, optional): The type of loss function being used. Defaults to 'bce'.\n",
    "\n",
    "    Returns:\n",
    "        DataLoader: The prepared dataset wrapped in a DataLoader.\n",
    "    \"\"\"\n",
    "    if shuffle:\n",
    "        ds = torch.utils.data.DataLoader(ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    else:\n",
    "        ds = torch.utils.data.DataLoader(ds, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "    if repeat:\n",
    "        ds = torch.utils.data.DataLoader(ds, batch_size=batch_size, shuffle=shuffle, drop_last=True)\n",
    "\n",
    "    if not predict:\n",
    "        ds = [(x, x) for x, y in ds]\n",
    "    else:\n",
    "        if model_loss_type == 'bce':\n",
    "            ds = [(x, y.view(-1, 1)) for x, y in ds]\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "def get_train_and_val_for_fold(folded_datasets, fold, MODEL_LOSS_TYPE, NFOLD, AUGMENT, PREDICT):\n",
    "    \"\"\"\n",
    "    Splits the provided datasets into training and validation sets for a given fold and prepares them for training.\n",
    "\n",
    "    Args:\n",
    "        folded_datasets (list): A list of datasets, where each dataset corresponds to a fold.\n",
    "        fold (int): The index of the fold to be used as the validation set.\n",
    "        MODEL_LOSS_TYPE (str): The type of model loss ('bce' for binary cross-entropy, 'mse' for mean squared error).\n",
    "        NFOLD (int): The total number of folds.\n",
    "        AUGMENT (bool): Whether to apply data augmentation to the training set.\n",
    "        PREDICT (bool): Whether the datasets are being prepared for prediction.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - train_ds: The training dataset prepared for training.\n",
    "            - val_ds: The validation dataset prepared for training.\n",
    "            - mse_val_ds: The validation dataset prepared for training if MODEL_LOSS_TYPE is 'mse', otherwise None.\n",
    "    \"\"\"\n",
    "\n",
    "    predict = True\n",
    "\n",
    "\n",
    "    if MODEL_LOSS_TYPE == 'bce':\n",
    "        train = []\n",
    "        for i in range(NFOLD):\n",
    "            if i == fold:\n",
    "                val_ds = folded_datasets[i]\n",
    "            else:\n",
    "                train.append(folded_datasets[i])\n",
    "    elif MODEL_LOSS_TYPE == 'mse':\n",
    "        train = []\n",
    "        for i in range(NFOLD):\n",
    "            if i == fold:\n",
    "                val_ds = folded_datasets[i][0].concatenate(folded_datasets[i][1])\n",
    "            else:\n",
    "                train.append(folded_datasets[i][0])\n",
    "\n",
    "    train_ds = None\n",
    "    for ds in train:\n",
    "        train_ds = ds if train_ds is None else train_ds.concatenate(ds)\n",
    "\n",
    "    mse_val_ds = None if not MODEL_LOSS_TYPE == 'mse' else prepare_for_training(val_ds, shuffle=False,  predict=True)\n",
    "    train_ds = prepare_for_training(train_ds, shuffle=True, repeat = True, predict=PREDICT, aug = AUGMENT)\n",
    "    val_ds = prepare_for_training(val_ds, shuffle=False,  predict=PREDICT)\n",
    "\n",
    "    return train_ds, val_ds, mse_val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "INPUT_COLUMNS = ['volt1',\n",
    " 'volt2',\n",
    " 'amp1',\n",
    " 'amp2',\n",
    " 'FQtyL',\n",
    " 'FQtyR',\n",
    " 'E1 FFlow',\n",
    " 'E1 OilT',\n",
    " 'E1 OilP',\n",
    " 'E1 RPM',\n",
    " 'E1 CHT1',\n",
    " 'E1 CHT2',\n",
    " 'E1 CHT3',\n",
    " 'E1 CHT4',\n",
    " 'E1 EGT1',\n",
    " 'E1 EGT2',\n",
    " 'E1 EGT3',\n",
    " 'E1 EGT4',\n",
    " 'OAT',\n",
    " 'IAS',\n",
    " 'VSpd',\n",
    " 'NormAc',\n",
    " 'AltMSL']\n",
    "\n",
    "\n",
    "\n",
    "class DataLoading:\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "        self.df = None # original dataframe\n",
    "\n",
    "    def load_data(self):\n",
    "        df_test = pd.read_csv(self.data_path, nrows=100)\n",
    "\n",
    "        float_cols = [c for c in df_test if df_test[c].dtype == \"float64\"]\n",
    "        # Change float32_cols to use float32 instead of float16\n",
    "        float32_cols = {c: np.float32 for c in float_cols}\n",
    "\n",
    "        df = pd.read_csv(self.data_path, engine='c', dtype=float32_cols)\n",
    "        df['id'] = df.id.astype('int32')\n",
    "        self.df = df.dropna() # you can handle nans differently, but ymmv\n",
    "        print(df.head(5))\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def min_max_scaling(self, input_columns, df=None):\n",
    "        df = self.df if df is None else df\n",
    "        qt = preprocessing.MinMaxScaler()\n",
    "        try:\n",
    "            qt.fit(df.loc[:, input_columns].sample(100000, random_state=0))\n",
    "        except ValueError as e:\n",
    "            if \"could not convert string to float\" in str(e):\n",
    "                problematic_string = str(e).split(\": \")[1].strip(\"'\")\n",
    "                print(f\"Error: Could not convert string to float: '{problematic_string}'\")\n",
    "\n",
    "                for col in input_columns:\n",
    "                    if problematic_string in df[col].astype(str).values:\n",
    "                        print(f\"Found problematic string in column: '{col}'\")\n",
    "\n",
    "            raise  # Re-raise the original exception after printing the information\n",
    "\n",
    "        arr = df.loc[:, input_columns].values\n",
    "        res = qt.transform(arr)\n",
    "\n",
    "        for i, col in tqdm(enumerate(input_columns)):\n",
    "            df.loc[:, col] = res[:, i]\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def get_folded_datasets(self,MODEL_LOSS_TYPE,df,NFOLD):\n",
    "        folded_datasets = []\n",
    "\n",
    "        for i in range(NFOLD):\n",
    "            if MODEL_LOSS_TYPE == 'bce':\n",
    "                folded_datasets.append(get_dataset(df[df.split == i]))\n",
    "            elif MODEL_LOSS_TYPE == 'mse':\n",
    "                after = get_dataset(df[(df.split == i) & (df.before_after == 1)])\n",
    "                before = get_dataset(df[(df.split == i) & (df.before_after == 0)])\n",
    "                folded_datasets.append((after, before))\n",
    "\n",
    "        return folded_datasets\n",
    "    \n",
    "\n",
    "    def get_train_and_val_for_fold(self,folded_datasets,fold,MODEL_LOSS_TYPE='bce',NFOLD=5,AUGMENT=None, PREDICT=False): \n",
    "        \n",
    "        if MODEL_LOSS_TYPE == 'bce':\n",
    "            train = []\n",
    "            for i in range(NFOLD):\n",
    "                if i == fold:\n",
    "                    val_ds = folded_datasets[i]\n",
    "                else:\n",
    "                    train.append(folded_datasets[i])\n",
    "        elif MODEL_LOSS_TYPE == 'mse':\n",
    "            train = []\n",
    "            for i in range(NFOLD):\n",
    "                if i == fold:\n",
    "                    val_ds = folded_datasets[i][0].concatenate(folded_datasets[i][1])\n",
    "                else:\n",
    "                    train.append(folded_datasets[i][0])\n",
    "\n",
    "        train_ds = None\n",
    "        for ds in train:\n",
    "            if isinstance(ds, torch.utils.data.TensorDataset):\n",
    "                ds_tensors = ds.tensors\n",
    "                for tensor in ds_tensors:\n",
    "                    train_ds = tensor if train_ds is None else torch.cat((train_ds, tensor))\n",
    "            else:\n",
    "                train_ds = ds if train_ds is None else torch.cat((train_ds, ds))\n",
    "\n",
    "        mse_val_ds = None if not MODEL_LOSS_TYPE == 'mse' else prepare_for_training(val_ds, shuffle=False,  predict=True)\n",
    "        train_ds = prepare_for_training(train_ds, shuffle=True, repeat = True, predict=PREDICT, aug = AUGMENT)\n",
    "        val_ds = prepare_for_training(val_ds, shuffle=False,  predict=PREDICT)\n",
    "\n",
    "        return train_ds, val_ds, mse_val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   volt1  volt2  amp1  amp2  FQtyL  FQtyR  E1 FFlow     E1 OilT    E1 OilP  \\\n",
      "0   27.9   27.9   7.9   0.7   24.0   24.0      2.09  129.199997  61.160000   \n",
      "1   27.9   27.9   7.9   0.6   24.0   24.0      2.13  129.199997  61.200001   \n",
      "2   27.9   28.0   8.0   0.6   24.0   24.0      2.07  129.199997  61.029999   \n",
      "3   27.9   27.9   7.8   0.6   24.0   24.0      2.12  129.199997  61.160000   \n",
      "4   27.9   27.9   7.7   0.6   24.0   24.0      2.08  129.100006  61.250000   \n",
      "\n",
      "   E1 RPM  ...  OAT  IAS       VSpd  NormAc  AltMSL  id  plane_id  split  \\\n",
      "0  1191.0  ...  7.2  0.0  15.740000   -0.02   822.5   2        18      4   \n",
      "1  1192.0  ...  7.2  0.0  11.130000   -0.00   822.5   2        18      4   \n",
      "2  1186.0  ...  7.2  0.0  -0.850000    0.00     NaN   2        18      4   \n",
      "3  1190.0  ...  7.2  0.0 -30.639999   -0.03   824.5   2        18      4   \n",
      "4  1197.0  ...  7.2  0.0 -23.950001   -0.02   825.0   2        18      4   \n",
      "\n",
      "   date_diff  before_after  \n",
      "0         -2             1  \n",
      "1         -2             1  \n",
      "2         -2             1  \n",
      "3         -2             1  \n",
      "4         -2             1  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manasdubey2022/anaconda3/envs/ngafid_env/lib/python3.9/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "0it [00:00, ?it/s]/var/folders/k_/mxrh7nj55tl1jtzf2n_d0swm0000gn/T/ipykernel_4254/4237317180.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, col] = res[:, i]\n",
      "23it [00:00, 27.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      volt1     volt2      amp1      amp2  FQtyL  FQtyR  E1 FFlow   E1 OilT  \\\n",
      "0  0.830769  0.828125  0.481117  0.600000    1.0    1.0  0.124405  0.567582   \n",
      "1  0.830769  0.828125  0.481117  0.577778    1.0    1.0  0.126786  0.567582   \n",
      "3  0.830769  0.828125  0.479475  0.577778    1.0    1.0  0.126190  0.567582   \n",
      "4  0.830769  0.828125  0.477832  0.577778    1.0    1.0  0.123810  0.567054   \n",
      "5  0.830769  0.828125  0.476190  0.577778    1.0    1.0  0.122619  0.567054   \n",
      "\n",
      "    E1 OilP    E1 RPM  ...       OAT       IAS      VSpd    NormAc    AltMSL  \\\n",
      "0  0.690678  0.419958  ...  0.569106  0.006946  0.582480  0.316832  0.030799   \n",
      "1  0.691124  0.420310  ...  0.569106  0.006946  0.582047  0.326733  0.030799   \n",
      "3  0.690678  0.419605  ...  0.569106  0.006946  0.578127  0.311881  0.030942   \n",
      "4  0.691682  0.422073  ...  0.569106  0.006946  0.578754  0.316832  0.030978   \n",
      "5  0.690232  0.419605  ...  0.566396  0.006946  0.580351  0.321782  0.030942   \n",
      "\n",
      "   id  plane_id  split  date_diff  before_after  \n",
      "0   2        18      4         -2             1  \n",
      "1   2        18      4         -2             1  \n",
      "3   2        18      4         -2             1  \n",
      "4   2        18      4         -2             1  \n",
      "5   2        18      4         -2             1  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 542/542 [00:01<00:00, 372.64it/s]\n",
      "100%|██████████| 417/417 [00:01<00:00, 408.88it/s]\n",
      "100%|██████████| 575/575 [00:01<00:00, 346.35it/s]\n",
      "100%|██████████| 418/418 [00:01<00:00, 341.37it/s]\n",
      "100%|██████████| 437/437 [00:01<00:00, 385.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.dataset.TensorDataset'>\n",
      "<torch.utils.data.dataset.TensorDataset object at 0x107539190>\n"
     ]
    }
   ],
   "source": [
    "data_path = \"/Users/manasdubey2022/Desktop/NGAFID/Codebase/data/NGAFID_MC_C37.csv\"\n",
    "data_loading = DataLoading(data_path)\n",
    "data = data_loading.load_data()\n",
    "data = data_loading.min_max_scaling(INPUT_COLUMNS)\n",
    "print(data.head(5))\n",
    "\n",
    "folded_datasets = data_loading.get_folded_datasets('bce',data,5)\n",
    "print(type(folded_datasets[0]))\n",
    "print(folded_datasets[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from fastprogress import progress_bar\n",
    "\n",
    "# Modify create_rocket_features to ensure correct tensor type and device\n",
    "def create_rocket_features_max(dl, model, device='cpu'):\n",
    "    model.eval()\n",
    "    _x_out = []\n",
    "    with torch.no_grad():\n",
    "        for xb in dl:\n",
    "            xb = xb.to(device).float()  # Ensure the input tensor is of the correct type and device\n",
    "            features = model(xb).cpu()\n",
    "            max_features = features[:, :model.n_kernels]  # Extract only the _max values\n",
    "            _x_out.append(max_features)\n",
    "    return torch.cat(_x_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsai.data.all import *\n",
    "from tsai.all import ROCKET\n",
    "import os\n",
    "\n",
    "# Set the environment variable for MPS fallback\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "BATCH_SIZE = 32 \n",
    "NUM_KERNELS = 10000\n",
    "KSS = [7, 9, 11]\n",
    "CHANNLES_IN=23\n",
    "SEQUENCE_LENGTH=4096    \n",
    "\n",
    "# Create a ROCKET model\n",
    "rocket = ROCKET(c_in=CHANNLES_IN, seq_len=SEQUENCE_LENGTH, n_kernels=NUM_KERNELS, kss=KSS).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Testing out on one fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 542\n",
      "Shape of each sample: torch.Size([542, 4096, 23])\n"
     ]
    }
   ],
   "source": [
    "one_fold = folded_datasets[0]\n",
    "print(f\"Number of samples: {len(one_fold)}\")\n",
    "print(f\"Shape of each sample: {one_fold.tensors[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "xb = torch.randn((32, 23, 4096))\n",
    "print(len(xb[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([224, 23, 4096])\n",
      "torch.Size([318, 23, 4096])\n"
     ]
    }
   ],
   "source": [
    "anomalies = []\n",
    "normal_data = []\n",
    "\n",
    "for data, label in one_fold:\n",
    "    if label == 1:\n",
    "        anomalies.append(data.permute(1, 0)) # change shape from (23, 4096) to (4096, 23)\n",
    "    else:\n",
    "        normal_data.append(data.permute(1, 0)) # change shape from (23, 4096) to (4096, 23)\n",
    "        \n",
    "#convert lists to Tensor datasets\n",
    "anomalies = torch.stack(anomalies)\n",
    "normal_data = torch.stack(normal_data)\n",
    "\n",
    "print(anomalies.shape)\n",
    "print(normal_data.shape)\n",
    "\n",
    "#Create data loaders\n",
    "anomalies_loader = torch.utils.data.DataLoader(anomalies, batch_size=32, shuffle=True)\n",
    "normal_data_loader = torch.utils.data.DataLoader(normal_data, batch_size=32, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches: 7\n",
      "Shape of each batch: torch.Size([32, 23, 4096])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of batches: {len(anomalies_loader)}\")\n",
    "for batch in anomalies_loader:\n",
    "\tprint(f\"Shape of each batch: {batch.shape}\")\n",
    "\tbreak  # Print the shape of the first batch and break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract features using the ROCKET model\n",
    "anomalous_features = create_rocket_features_max(anomalies_loader, rocket, device=device)\n",
    "normal_features = create_rocket_features_max(normal_data_loader, rocket, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([318, 10000])\n"
     ]
    }
   ],
   "source": [
    "print(normal_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "#Convert features to numpy arrays\n",
    "anomalous_features = anomalous_features.numpy()\n",
    "normal_features = normal_features.numpy()\n",
    "\n",
    "# Fit the OC-SVM model on normal features\n",
    "svm = OneClassSVM(kernel='rbf', gamma=0.001, nu=0.02)\n",
    "svm.fit(normal_features)\n",
    "\n",
    "# Predict anomalies in the anomalous features\n",
    "pred = svm.predict(anomalous_features)\n",
    "scores = svm.score_samples(anomalous_features)\n",
    "\n",
    "# Calculate the threshold value for anomalies\n",
    "thresh = np.quantile(scores, 0.03)\n",
    "print(\"Threshold for anomalies:\", thresh)\n",
    "\n",
    "# Extract the anomalies\n",
    "index = np.where(scores <= thresh)\n",
    "anomalies = anomalous_features[index]\n",
    "\n",
    "# Testing the SVM model on normal and anomalous features\n",
    "test_normal_pred = svm.predict(normal_features)\n",
    "test_anomalous_pred = svm.predict(anomalous_features)\n",
    "\n",
    "# Calculate accuracy for normal and anomalous data\n",
    "normal_accuracy = np.mean(test_normal_pred == 1)\n",
    "anomalous_accuracy = np.mean(test_anomalous_pred == -1)\n",
    "\n",
    "print(f\"Accuracy on normal data: {normal_accuracy * 100:.2f}%\")\n",
    "print(f\"Accuracy on anomalous data: {anomalous_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies = [] # post maintenance (0)\n",
    "normal_data = [] # pre maintenance (1)\n",
    "for fold in folded_datasets:\n",
    "    for data, label in fold:\n",
    "        if label == 1:\n",
    "            normal_data.append(data.permute(1, 0)) # change shape from (23, 4096) to (4096, 23)\n",
    "        else:\n",
    "            anomalies.append(data.permute(1, 0)) # change shape from (23, 4096) to (4096, 23)\n",
    "\n",
    "anomlies_dataset = torch.stack(anomalies)\n",
    "normal_dataset = torch.stack(normal_data)\n",
    "\n",
    "anomalies_loader = torch.utils.data.DataLoader(anomlies_dataset, batch_size=32, shuffle=True)\n",
    "normal_loader = torch.utils.data.DataLoader(normal_dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches: 45\n",
      "Shape of each batch: torch.Size([32, 23, 4096])\n"
     ]
    }
   ],
   "source": [
    "#Reveal properties of the DataLoaders so i can make tests in the future\n",
    "print(f\"Number of batches: {len(anomalies_loader)}\")\n",
    "for batch in anomalies_loader:\n",
    "\tprint(f\"Shape of each batch: {batch.shape}\")\n",
    "\tbreak  # Print the shape of the first batch and break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalous_features = create_rocket_features_max(anomalies_loader, rocket, device=device)\n",
    "normal_features = create_rocket_features_max(normal_loader, rocket, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1413, 10000])\n",
      "torch.Size([976, 10000])\n"
     ]
    }
   ],
   "source": [
    "print(anomalous_features.shape)\n",
    "print(normal_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "\n",
    "# Define the number of folds and repetitions\n",
    "\n",
    "k = 5\n",
    "repetitions = 10\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'kernel': ['rbf', 'linear', 'poly', 'sigmoid'],\n",
    "    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],\n",
    "    'nu': [0.01, 0.05, 0.1, 0.2, 0.5]\n",
    "}\n",
    "\n",
    "# Define the custom scoring function for GridSearchCV\n",
    "def custom_f1_score(y_true, y_pred):\n",
    "    return f1_score(y_true, y_pred, pos_label=-1)\n",
    "\n",
    "scorer = make_scorer(custom_f1_score, greater_is_better=True)\n",
    "\n",
    "grid_search = GridSearchCV(OneClassSVM(), param_grid, scoring=scorer, cv=5, n_jobs=-1)\n",
    "\n",
    "#Fit the grid search on normal features\n",
    "grid_search.fit(normal_features)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "f1_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalous_features = anomalous_features.numpy()\n",
    "normal_features = normal_features.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OC-SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1-score: 0.9340\n",
      "Standard deviation of F1-score: 0.0003\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "\n",
    "for train_index, test_index in kf.split(normal_features):\n",
    "    for _ in range(repetitions):\n",
    "        # Split the normal features into training and validation sets\n",
    "        X_train, X_test_normal = normal_features[train_index], normal_features[test_index]\n",
    "\n",
    "        # Combine the validation set with the anomalous features to form the test set\n",
    "        X_test = np.vstack((X_test_normal, anomalous_features))\n",
    "        y_test = np.hstack((np.ones(len(X_test_normal)), -np.ones(len(anomalous_features))))\n",
    "\n",
    "        # Fit the OC-SVM model on the training set\n",
    "        svm = OneClassSVM(kernel=best_params['kernel'], gamma=best_params['gamma'], nu=best_params['nu']) \n",
    "\n",
    "        # Calculate the anomaly scores for the training data\n",
    "        train_scores = svm.score_samples(X_train)\n",
    "\n",
    "        # Compute the mean and standard deviation of the anomaly scores\n",
    "        mean_train_score = np.mean(train_scores)\n",
    "        std_train_score = np.std(train_scores)\n",
    "\n",
    "        # Calculate the threshold using the formula\n",
    "        tau = mean_train_score - 3 * std_train_score\n",
    "\n",
    "        # Calculate the anomaly scores for the test data\n",
    "        test_scores = svm.score_samples(X_test)\n",
    "\n",
    "        # Predict anomalies using the threshold\n",
    "        y_pred = np.where(test_scores <= tau, -1, 1)\n",
    "\n",
    "        # Calculate the F1-score\n",
    "        f1 = f1_score(y_test, y_pred, pos_label=-1)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "# Calculate the average F1-score and standard deviation\n",
    "avg_f1_score = np.mean(f1_scores)\n",
    "std_f1_score = np.std(f1_scores)\n",
    "\n",
    "print(f\"Average F1-score: {avg_f1_score:.4f}\")\n",
    "print(f\"Standard deviation of F1-score: {std_f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Isol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ngafid_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
